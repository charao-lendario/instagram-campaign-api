# Story 1.3: Endpoint de Scraping -- Buscar Comentários de Cada Post via Apify

## Status: InProgress
## Epic: 1 - Backend (FastAPI + Apify)
## Points: 5

---

## Description

Como analista de campanha, quero que todos os comentários de cada post coletado sejam buscados automaticamente via Apify `instagram-comment-scraper`, para que análise de sentimento e extração de temas tenham dados completos para processar.

Esta story estende o serviço de scraping com a função de coleta de comentários, incluindo mapeamento dos campos do Apify para o modelo `Comment`, upsert idempotente (deduplicação por `instagram_id`), tratamento de falhas parciais (falha em um post não para o processo), atualização do `ScrapingRun`, e o endpoint de trigger.

---

## Acceptance Criteria

- [x] **AC1**: Given que posts existem no banco, when `scrape_comments(post_url: str, post_id: UUID, run_id: UUID) -> list[Comment]` é chamado, then o ator `apify/instagram-comment-scraper` é invocado com input `{"directUrls": [post_url], "resultsLimit": 500}`, e o resultado é uma lista de comentários mapeados para o modelo `Comment`.

- [x] **AC2**: Given que o Apify retorna dados de um comentário, when o mapeamento é executado, then os seguintes campos são corretamente mapeados para `CommentUpsert`:
  - `id` (Apify) → `instagram_id`
  - `text` → `text`
  - `ownerUsername` → `author_username`
  - `timestamp` → `commented_at`
  - `likesCount` → `like_count`
  - `replies` (count ou lista) → `reply_count`
  - Objeto completo Apify → `raw_data` (JSONB)
  - `post_id` passado como parâmetro
  - `scraping_run_id` do run corrente

- [x] **AC3**: Given que um comentário já existe no banco (mesmo `instagram_id`), when `scrape_comments()` é chamado novamente, then o comentário é atualizado (upsert) com o novo valor de `like_count` e `scraped_at`. Nenhum registro duplicado é criado.

- [x] **AC4**: Given uma lista de posts, when `scrape_all_comments(posts: list[Post], run_id: UUID) -> int` é chamado, then: (a) `scrape_comments()` é invocado sequencialmente para cada post; (b) se um post falha, o erro é logado e adicionado ao `scraping_runs.errors`, mas o processo continua para os posts restantes; (c) o total de comentários coletados com sucesso é retornado; (d) `scraping_runs.comments_scraped` é atualizado com o total.

- [x] **AC5**: Given que o ator Apify falha para um post específico (timeout, rate limit), when a exceção é capturada em `scrape_comments()`, then: (a) o erro é logado com `post_url`, `post_id`, `actor_id`, e `error_message` em structured logging; (b) o erro é adicionado ao `scraping_runs.errors` JSONB com `phase: "comment_scraping"`; (c) a função retorna lista vazia `[]` para esse post; (d) `scrape_all_comments()` continua para os próximos posts.

- [x] **AC6**: Given que `POST /api/v1/scraping/comments` é chamado sem body, when processado, then: (a) os posts elegíveis são identificados (todos os posts existentes no banco ou posts com `scraped_at` mais antigo que o threshold de staleness configurado -- default: todos os posts do run mais recente); (b) `scrape_all_comments()` é invocado; (c) a resposta `202 Accepted` é retornada com `{"run_id": "<uuid>", "status": "started", "posts_queued": <N>}`.

- [x] **AC7**: Given que os unit tests existem em `tests/test_scraping.py`, when `pytest tests/test_scraping.py -k "comment"` é executado, then todos passam. Os testes cobrem:
  - Scraping bem-sucedido de comentários com mock do `apify-client` retornando 50 comentários
  - Deduplicação: segundo scraping do mesmo comentário atualiza ao invés de duplicar
  - Falha parcial: falha em 1 de 3 posts -- os outros 2 são processados, erro logado
  - `scrape_all_comments()`: retorna total correto de comentários coletados

---

## Scope

### IN
- `app/services/scraping.py` -- adicionar `scrape_comments()` e `scrape_all_comments()`
- Helper interno para mapear resposta Apify de comentário para `CommentUpsert`
- Upsert de comentários via `supabase.table("comments").upsert(..., on_conflict="instagram_id")`
- Atualização de `ScrapingRun.comments_scraped` e `errors`
- `app/routers/scraping.py` -- adicionar `POST /api/v1/scraping/comments`
- `tests/test_scraping.py` -- seção de testes de comment scraping

### OUT
- Análise de sentimento dos comentários (Stories 1.4, 1.5)
- Extração de temas (Story 1.6)
- Scraping em paralelo (MVP é sequencial por post)
- Paginação de comentários além de `resultsLimit: 500`

---

## Dependencies

- Story 1.1 concluída (configuração, modelos, cliente Supabase)
- Story 1.2 concluída (posts devem existir no banco para buscar comentários)
- Conta Apify com acesso ao ator `apify/instagram-comment-scraper`

---

## Technical Notes

### Apify Comment Scraper Input (architecture.md Seção 6.1)

```python
run = client.actor("apify/instagram-comment-scraper").call(
    run_input={
        "directUrls": [post_url],
        "resultsLimit": 500
    }
)
items = list(client.dataset(run["defaultDatasetId"]).iterate_items())
```

### Lógica de Falha Parcial (NFR-005)

O loop em `scrape_all_comments()` usa try/except individual por post:
```python
for post in posts:
    try:
        comments = scrape_comments(post.url, post.id, run_id)
        total_scraped += len(comments)
    except Exception as e:
        logger.error("comment_scraping_failed", post_id=str(post.id), error=str(e))
        # append to errors JSONB, continue loop
```

### Upsert de Comentários (SCHEMA.md Seção 9)

```python
supabase.table("comments").upsert(
    comment_data_dict,
    on_conflict="instagram_id"
).execute()
```

O upsert atualiza: `like_count`, `scraped_at`.

### Endpoints Envolvidos (architecture.md Seção 3.2.2)

- `POST /api/v1/scraping/comments` -- 202 Accepted
  - Response: `{"run_id": "uuid", "status": "started", "posts_queued": 20}`

### Tabelas Envolvidas
- `posts` (SELECT -- buscar posts para coletar comentários)
- `comments` (UPSERT ON CONFLICT instagram_id)
- `scraping_runs` (UPDATE comments_scraped + UPDATE errors)

### Custo Apify (NFR-009)
- 1 actor run por post = 20 runs por ciclo completo (10 posts x 2 candidatos)
- `resultsLimit: 500` por post alinhado com NFR-004 (estimativa de ~500 comentários/post)
- Actor ID configurável: `APIFY_COMMENT_ACTOR_ID` (default: `"apify/instagram-comment-scraper"`)

### PRD References
- FR-002: Instagram Comment Scraping -- todos os comentários de cada post via instagram-comment-scraper
- CON-001: Scraping exclusivamente via Apify
- NFR-004: Escala -- até 500 comentários por post
- NFR-005: Error Resilience -- falha parcial preservada, processo continua
- NFR-009: Cost Efficiency -- actor runs controlados

---

## Risks

| Risco | Probabilidade | Impacto | Mitigacao |
|-------|---------------|---------|-----------|
| Volume alto de actor runs (20 por ciclo: 10 posts x 2 candidatos) | Media | Alto | resultsLimit: 500 controla volume por post; monitorar custo via scraping_runs; actor ID configuravel (APIFY_COMMENT_ACTOR_ID) |
| Rate limiting do Apify com 20 chamadas sequenciais | Media | Medio | Scraping sequencial (nao paralelo) reduz pico; falha parcial (AC4/AC5) preserva dados coletados; retry no proximo ciclo |
| Timeout em posts com muitos comentarios (>500) | Baixa | Medio | resultsLimit: 500 limita volume; actor.call() bloqueante aceitavel para MVP |
| Mudanca na estrutura de dados do comment-scraper | Baixa | Alto | raw_data preserva resposta original; helper de mapeamento isolado facilita ajuste |
| Falha em cascata se Story 1.2 nao criou posts | Baixa | Alto | Dependencia explicita em 1.2; endpoint verifica posts existentes antes de iniciar |

---

## File List

| Arquivo | Acao | Status |
|---------|------|--------|
| `app/services/scraping.py` | Modificar -- adicionar `scrape_comments()` e `scrape_all_comments()` | Implementado (na mesma criacao da Story 1.2) |
| `app/routers/scraping.py` | Modificar -- adicionar `POST /api/v1/scraping/comments` | Implementado (na mesma criacao da Story 1.2) |
| `tests/test_scraping.py` | Modificar -- adicionar testes de comment scraping | Implementado (11 testes de comments) |

---

## Dev Notes

- `_map_apify_comment()` trata `replies` como lista (len) ou int (direto), cobrindo ambos os formatos do Apify
- Para MVP, `_get_eligible_posts()` busca TODOS os posts no banco (sem filtro de staleness) -- simplificacao aceitavel
- `scrape_comments()` retorna `[]` em caso de falha Apify (resiliencia parcial) em vez de propagar excecao
- `scrape_all_comments()` loga progresso "N/M" para cada post processado
- `APIFY_COMMENT_ACTOR_ID` configuravel via settings (default: `apify/instagram-comment-scraper`)
- Endpoint retorna `comments_scraped` adicional no response body alem do que AC6 pede (informacao util)

---

## Change Log

| Data | Autor | Descricao |
|------|-------|-----------|
| 2026-02-21 | River (SM) | Story criada |
| 2026-02-21 | Pax (PO) | Validacao: GO 9/10. Adicionada secao Risks. Status Draft -> Ready |
| 2026-02-21 | Dex (Dev) | Implementacao completa: scrape_comments, scrape_all_comments, endpoint. Status Ready -> InProgress. Todos ACs atendidos. |

---

## Validation

| # | Criterio | Resultado |
|---|----------|-----------|
| 1 | Titulo claro e objetivo | PASS |
| 2 | Descricao completa | PASS |
| 3 | ACs testaveis (Given/When/Then) | PASS |
| 4 | Escopo bem definido (IN/OUT) | PASS |
| 5 | Dependencias mapeadas | PASS |
| 6 | Estimativa de complexidade | PASS |
| 7 | Valor de negocio | PASS |
| 8 | Riscos documentados | PASS (adicionados durante validacao) |
| 9 | Criterios de Done | PASS |
| 10 | Alinhamento com PRD/Epic | PASS |

**Score: 9/10 -> GO**
**Validador:** Pax (PO)
**Data:** 2026-02-21
**Observacoes:** Secao Risks estava ausente e foi adicionada durante validacao. 20 actor runs por ciclo e o principal risco de custo -- NFR-009 exige monitoramento. Campos do AC2 alinhados com SCHEMA.md (reply_count, raw_data, scraping_run_id). PRD References verificados: FR-002, CON-001, NFR-004, NFR-005, NFR-009.
