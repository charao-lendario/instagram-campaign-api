# Story 1.2: Endpoint de Scraping -- Buscar Últimos 10 Posts via Apify

## Status: Draft
## Epic: 1 - Backend (FastAPI + Apify)
## Points: 5

---

## Description

Como analista de campanha, quero que o sistema colete automaticamente os últimos 10 posts de cada perfil monitorado (@charlles.evangelista e @delegadasheila) usando o ator `apify/instagram-post-scraper`, para que eu tenha dados frescos de posts disponíveis para análise de engajamento.

Esta story implementa o serviço de scraping de posts, incluindo mapeamento dos campos retornados pelo Apify para o modelo `Post`, upsert idempotente no Supabase (deduplicação por `instagram_id`), registro de `scraping_runs`, tratamento de erros resiliente, e o endpoint de trigger.

---

## Acceptance Criteria

- [ ] **AC1**: Given que `APIFY_TOKEN` está configurado, when `scrape_posts(candidate_username: str) -> list[Post]` é chamado com `"charlles.evangelista"`, then o ator `apify/instagram-post-scraper` é invocado via `apify-client` com input `{"usernames": [candidate_username], "resultsLimit": 10}`, e o resultado é uma lista de no máximo 10 posts mapeados para o modelo `Post`.

- [ ] **AC2**: Given que o Apify retorna dados de um post, when o mapeamento é executado, then os seguintes campos são corretamente mapeados para `PostUpsert`:
  - `id` (Apify) → `instagram_id`
  - `url` ou `postUrl` → `url`
  - `shortCode` → `shortcode`
  - `caption` → `caption`
  - `likesCount` → `like_count`
  - `commentsCount` → `comment_count`
  - `type` → `media_type` (mapeado para enum: image/video/carousel/unknown)
  - `isSponsored` → `is_sponsored`
  - `videoViewCount` → `video_view_count`
  - `timestamp` → `posted_at`
  - Objeto completo Apify → `raw_data` (JSONB)
  - `candidate_id` resolvido a partir do `username` via query em `candidates`
  - `scraping_run_id` associado ao `ScrapingRun` corrente

- [ ] **AC3**: Given que um post já existe no banco (mesmo `instagram_id`), when `scrape_posts()` é chamado novamente, then o post é atualizado (upsert) com os novos valores de `like_count`, `comment_count`, `scraped_at`, e `updated_at`. Nenhum registro duplicado é criado.

- [ ] **AC4**: Given que um `ScrapingRun` com status `running` é criado antes do scraping iniciar, when `scrape_posts()` completa com sucesso, then `scraping_runs.posts_scraped` é incrementado com o número de posts retornados e o status permanece `running` (será fechado pelo pipeline completo na Story 1.7).

- [ ] **AC5**: Given que o ator Apify falha (timeout, rate limit, actor error), when `scrape_posts()` captura a exceção, then: (a) a exceção é logada com structured logging contendo `candidate_username`, `actor_id`, e `error_message`; (b) o erro é adicionado ao `scraping_runs.errors` JSONB com campos `candidate`, `phase: "post_scraping"`, `message`, e `timestamp`; (c) uma exceção descritiva é propagada para o chamador; (d) a aplicação NÃO crasha.

- [ ] **AC6**: Given que `POST /api/v1/scraping/posts` é chamado sem body, when processado, then: (a) um novo `ScrapingRun` é criado com `status: "running"`; (b) `scrape_posts()` é invocado para cada candidato ativo (`is_active = True`) em `candidates`; (c) a resposta `202 Accepted` é retornada com `{"run_id": "<uuid>", "status": "started", "candidates": ["charlles.evangelista", "delegadasheila"]}`; (d) o scraping pode continuar em background (mas para MVP pode ser síncrono dado que Railway não tem limite de request timeout restritivo).

- [ ] **AC7**: Given que os unit tests existem em `tests/test_scraping.py`, when `pytest tests/test_scraping.py -k "post"` é executado, then todos passam. Os testes cobrem:
  - Scraping bem-sucedido com mock do `apify-client` retornando 10 posts
  - Deduplicação: segundo scraping do mesmo post atualiza ao invés de duplicar
  - Falha do ator Apify: exceção é propagada corretamente e erros logados
  - Mapeamento correto de campos (especialmente `media_type` e campos opcionais)

---

## Scope

### IN
- `app/services/scraping.py` -- função `scrape_posts(candidate_username: str, run_id: UUID) -> list[Post]`
- Helper interno para resolver `candidate_id` a partir de `username`
- Helper interno para mapear resposta Apify para `PostUpsert`
- Upsert de posts via `supabase.table("posts").upsert(..., on_conflict="instagram_id")`
- Criação e atualização de `ScrapingRun`
- `app/routers/scraping.py` -- endpoint `POST /api/v1/scraping/posts`
- `tests/test_scraping.py` -- seção de testes de post scraping

### OUT
- Scraping de comentários (Story 1.3)
- Pipeline completo (Story 1.7)
- Análise de sentimento (Stories 1.4, 1.5)
- Scraping paralelo/assíncrono avançado (MVP é síncrono sequencial)

---

## Dependencies

- Story 1.1 concluída (configuração, modelos, cliente Supabase)
- Supabase com seed data dos candidatos (`charlles.evangelista`, `delegadasheila`)
- Conta Apify com token válido e acesso ao ator `apify/instagram-post-scraper`

---

## Technical Notes

### Apify Client Pattern (architecture.md Seção 6.1)

```python
from apify_client import ApifyClient
from app.core.config import settings

def _get_apify_client() -> ApifyClient:
    return ApifyClient(settings.APIFY_TOKEN)

# Actor invocation (synchronous -- waits for completion)
client = _get_apify_client()
run = client.actor("apify/instagram-post-scraper").call(
    run_input={
        "usernames": [candidate_username],
        "resultsLimit": 10
    }
)
items = list(client.dataset(run["defaultDatasetId"]).iterate_items())
```

### Upsert Pattern (SCHEMA.md Seção 9)

```python
supabase.table("posts").upsert(
    post_data_dict,
    on_conflict="instagram_id"
).execute()
```

O upsert atualiza: `like_count`, `comment_count`, `scraped_at`, `updated_at`.

### Estrutura do ScrapingRun (SCHEMA.md Seção 3.2)

Campos do `errors` JSONB:
```json
[
  {
    "candidate": "charlles.evangelista",
    "phase": "post_scraping",
    "message": "Apify actor timeout after 60s",
    "timestamp": "2026-02-21T10:00:00Z"
  }
]
```

### Endpoints Envolvidos (architecture.md Seção 3.2.2)

- `POST /api/v1/scraping/posts` -- 202 Accepted
  - Response: `{"run_id": "uuid", "status": "started", "candidates": [...]}`

### Tabelas Envolvidas
- `candidates` (SELECT WHERE is_active = TRUE)
- `posts` (UPSERT ON CONFLICT instagram_id)
- `scraping_runs` (INSERT + UPDATE posts_scraped + UPDATE errors)

### PRD References
- FR-001: Instagram Post Scraping -- 10 posts por candidato via instagram-post-scraper
- CON-001: Scraping exclusivamente via Apify
- CON-002: Exatamente 2 perfis monitorados (MVP)
- NFR-005: Error Resilience -- falha não crasha, erros logados
- NFR-009: Cost Efficiency -- máximo 2 actor runs para posts (1 por candidato)

### Considerações de Custo (NFR-009)
- 1 actor run por candidato = 2 runs por ciclo de scraping de posts
- `resultsLimit: 10` garante que não ultrapassa o necessário
- ID do ator deve ser configurável via settings (`APIFY_POST_ACTOR_ID`, default: `"apify/instagram-post-scraper"`)

---

## File List

| Arquivo | Acao |
|---------|------|
| `app/services/scraping.py` | Criar -- `scrape_posts()` e helpers |
| `app/routers/scraping.py` | Criar -- `POST /api/v1/scraping/posts` |
| `app/main.py` | Modificar -- registrar `scraping.router` |
| `tests/test_scraping.py` | Criar -- testes de post scraping |

---

## Dev Notes

<!-- @dev: Espaço para anotar decisões durante implementação -->
<!-- O APIFY_POST_ACTOR_ID deve ser adicionado ao config.py como setting com default -->
<!-- actor.call() é bloqueante -- para MVP está OK, mas para production Railway pode precisar de timeout configurado -->
<!-- O campo raw_data deve armazenar o objeto completo do Apify como dict -- útil para debug -->
<!-- Verificar se o ator retorna "id" ou outra chave como instagram_id -- raw_data preserva os dados originais para inspeção -->

---

## Change Log

| Data | Autor | Descricao |
|------|-------|-----------|
| 2026-02-21 | River (SM) | Story criada |
